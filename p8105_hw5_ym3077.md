p8105_hw5_ym3077
================

Load package

``` r
library(tidyverse)
```

    ## â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€
    ## âœ” dplyr     1.1.4     âœ” readr     2.1.5
    ## âœ” forcats   1.0.0     âœ” stringr   1.5.1
    ## âœ” ggplot2   3.5.2     âœ” tibble    3.3.0
    ## âœ” lubridate 1.9.4     âœ” tidyr     1.3.1
    ## âœ” purrr     1.1.0     
    ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
    ## âœ– dplyr::filter() masks stats::filter()
    ## âœ– dplyr::lag()    masks stats::lag()
    ## â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(broom)
set.seed(1)
```

## Problem 1

Write a function that simulates birthdays and checks for duplicates

``` r
bday_sim = function(n_people) {
  
  birthdays = sample(1:365, size = n_people, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_people

  repeated_bday
  
}
```

Run this function 10000 times for each group size between 2 and 50. For
each group size, compute the probability that at least two people in the
group will share a birthday by averaging across the 10000 simulation
runs.

``` r
bday_results = 
  expand_grid(
    group_size = 2 :50,
    iter = 1:1000) |>
  mutate(share_bday = map_lgl(group_size, bday_sim)) |>
  group_by(group_size) |>
  summarize(prob_sharebday = mean(share_bday))
```

Make a plot showing the probability as a function of group size, and
comment on your results.

``` r
bday_results |> 
  ggplot(aes(x = group_size, y = prob_sharebday)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability of Shared Birthday vs. Sample Size",
    x = "Number of People in the Room",
    y = "Probability of more than 1 Shared Birthday"
  ) +
  theme_minimal()
```

![](p8105_hw5_ym3077_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

This plot shows the estimated probability of at least 2 people in a
group share the same birthday, for group sizes varying from 2 to 50. The
probability is very low for small groups (less than 5), and rises
rapidly for groups with 15-30 people, finally approaches to 1 as the
group size nears 50.

## Problem 2

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value ofğœ‡on the x axis.
Describe the association between effect size and power.

Function Set up and Run 5000 simulations for Î¼ = 0,1,2,3,4,5,6

``` r
n = 30
sigma = 5
sim_one <- function(mu) {
  x <- rnorm(n = n, mean = mu, sd = sigma)
  ttest <- t.test(x, mu = 0)
  tidy(ttest) |>
    select(estimate, p.value) |>
    rename(mu_hat = estimate) 
}

sim_results <-
  expand_grid(
    true_mu = 0:6,
    iter = 1:5000) |>
  mutate(output = map(true_mu, sim_one)) |>
  unnest(output)
```

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value of ğœ‡ on the x axis.
Describe the association between effect size and power.

``` r
power_df <-
  sim_results |>
  group_by(true_mu) |>
  summarize(power = mean (p.value <0.5)) |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_point() +
  geom_line() +
  labs( titble = "Power of One-Sample T-Test",
        x = "True mean (Î¼)",
        y = "Power: Proportion of times reject H0"
  )
power_df
```

![](p8105_hw5_ym3077_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

The plot shows power increases as the true mean (Î¼) increases. Since the
H0: Î¼ = 0, and population standard deviation is fixed at 5, as true mean
(Î¼) increases to 1, 2, 3 and higher, the effect size increases, the test
is increasingly likely to detect the mean is not zero, and power rises
sharply approaching to 1 for Î¼ = 4.

Make a plot showing the average estimate of ğœ‡Ì‚ on the y axis and the true
value of ğœ‡ on the x axis.

``` r
avg_est <- sim_results |>
  group_by(true_mu) |>
  summarize(avg_mu_hat = mean(mu_hat))

ggplot(avg_est, aes(x = true_mu, y = avg_mu_hat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Î¼ vs. True mean (Î¼)",
    x = "True mean (Î¼)",
    y = "Average Î¼Ì‚")+
  theme_minimal()
```

![](p8105_hw5_ym3077_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Make a second plot (or overlay on the first) the average estimate of ğœ‡Ì‚
only in samples for which the null was rejected on the y axis and the
true value of ğœ‡ on the x axis.

``` r
avg_reject <-
  sim_results |>
  filter(p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(mu_hat_reject = mean(mu_hat))

ggplot(avg_reject, aes(x = true_mu, y = mu_hat_reject)) +
  geom_point() +
  geom_line() +
  labs(
    title = " Average Î¼Ì‚ when H0 is rejected",
    x = "True mean (Î¼)",
    y = "Average Î¼Ì‚ on rejected samples"
  ) +
  theme_minimal()
```

![](p8105_hw5_ym3077_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

Is the sample average of ğœ‡Ì‚ across tests for which the null is rejected
approximately equal to the true value of ğœ‡? Why or why not? No,
according to the plot the sample average of Î¼_hat among only the
rejected samples is not equal to the true mean (Î¼), especially when true
mean (Î¼) is small. Because rejecting H0 requires extreme estimates
(sample mean to be far enough from 0) to produce a small p-value. As
true mean (Î¼) gets larger and the test has higher power, nearly all
samples are included in the â€œrejectingâ€ region, and all reject the null.

## Problem 3

Upload the dataset.

``` r
homicides <- 
  read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv")
```

Describe the raw data. Create a `city_state` variable (e.g.Â â€œBaltimore,
MDâ€) and then summarize within cities to obtain the total number of
homicides and the number of unsolved homicides (those for which the
disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

``` r
homicides_sum <- homicides |>
  mutate(city_state = str_c(city, "," , state),
         unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved),
    )
homicides_sum
```

    ## # A tibble: 51 Ã— 3
    ##    city_state     total_homicides unsolved_homicides
    ##    <chr>                    <int>              <int>
    ##  1 Albuquerque,NM             378                146
    ##  2 Atlanta,GA                 973                373
    ##  3 Baltimore,MD              2827               1825
    ##  4 Baton Rouge,LA             424                196
    ##  5 Birmingham,AL              800                347
    ##  6 Boston,MA                  614                310
    ##  7 Buffalo,NY                 521                319
    ##  8 Charlotte,NC               687                206
    ##  9 Chicago,IL                5535               4073
    ## 10 Cincinnati,OH              694                309
    ## # â„¹ 41 more rows

The raw dataset contains 52179 homicide records collected by The
Washington Post from 50 large U.S. cities. Each row orresponds to a
single homicide case and includes detailed information onâ€ uid, date,
reported, victim demographics, the city and state, latitude/longitude,
and the case disposition (solved or unsolved).

For the city of Baltimore, MD, use the `prop.test` function to estimate
the proportion of homicides that are unsolved; save the output of
`prop.test` as an R object, apply the `broom::tidy` to this object and
pull the estimated proportion and confidence intervals from the
resulting tidy dataframe.

``` r
baltimore = homicides_sum |>
  filter(city_state == "Baltimore,MD")

baltimore_test =
  prop.test(
    x = baltimore$unsolved_homicides,
    n = baltimore$total_homicides
  )

baltimore_tidy = 
  tidy(baltimore_test) |>
  select(estimate, conf.low, conf.high)
  
baltimore_tidy
```

    ## # A tibble: 1 Ã— 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

The estimated proportion of unsolved homicides in Baltimore, MD is 0.646
with a 95% CI of (0.628, 0.663).

Now run `prop.test` for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a â€œtidyâ€ pipeline, making use of `purrr::map`,
`purrr::map2`, list columns and `unnest` as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
city_prop = homicides_sum |>
  mutate(
    test = map2(unsolved_homicides, total_homicides, ~ prop.test(x = .x, n = .y)),
    tidy = map(test, broom::tidy)) |>
  select(city_state, tidy) |>
  unnest(tidy)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## â„¹ In argument: `test = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

Create a plot that shows the estimates and CIs for each city â€“ check out
`geom_errorbar` for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
city_prop_plot = city_prop |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "red", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion Unsolved"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
city_prop_plot
```

![](p8105_hw5_ym3077_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->
